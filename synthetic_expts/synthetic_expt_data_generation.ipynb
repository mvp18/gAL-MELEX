{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ZSL target-shift: synthetic data generation\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import os, sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "### Synthetic data generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feat = 5\n",
    "num_attr = 2\n",
    "cov_matrix = np.eye(num_feat)  # a common cov mat for all distributions.\n",
    "\n",
    "means = []\n",
    "for i in range(num_attr):\n",
    "    x = np.random.multivariate_normal(np.zeros(num_feat), cov_matrix)\n",
    "    x = np.array([x, x+np.ones(num_feat)])    \n",
    "    means.append(x)\n",
    "    \n",
    "# distance between means can be used to make classification task easy or hard. Larger the distances easier the task\n",
    "def mean_alteration(means, d1=1, d2=1):\n",
    "    # ratio: ratio between the means of second task to first task\n",
    "    dis1 = np.linalg.norm(means[0][0] - means[0][1])\n",
    "    dis2 = np.linalg.norm(means[1][0] - means[1][1])\n",
    "    means[0][1] = means[0][0] + d1*(means[0][1] - means[0][0])/dis1\n",
    "    means[1][1] = means[1][0] + d2*(means[1][1] - means[1][0])/dis2\n",
    "    return means\n",
    "means_alt = mean_alteration(means, 1.5, 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-occurence of classes. This ratio varies for train and test set\n",
    "prob0 = 0.5\n",
    "def get_conditional_prob(corr_prob):\n",
    "    cond_mat = np.zeros((num_attr, num_attr))\n",
    "    cond_mat[0,0] = corr_prob\n",
    "    cond_mat[0,1] = 1 - cond_mat[0,0]\n",
    "    cond_mat[1,0] = 1 - cond_mat[0,0]\n",
    "    cond_mat[1,1] = cond_mat[0,0]\n",
    "    return cond_mat\n",
    "\n",
    "cond_best = 0.5 * np.ones((num_attr, num_attr))\n",
    "\n",
    "def gen_data_cond(num_data, cond_prob, means):\n",
    "    X = []\n",
    "    Y = []\n",
    "    prob0 = 0.5\n",
    "    for i in range(num_data):\n",
    "        y0 = int(np.random.rand() < prob0)\n",
    "        y1 = int(np.random.rand() < cond_prob[y0, 1])\n",
    "        Y.append([y0, y1])\n",
    "\n",
    "        x0 = np.random.multivariate_normal(means[0][y0], np.eye(num_feat), 1)\n",
    "        x1 = np.random.multivariate_normal(means[1][y1], np.eye(num_feat), 1)\n",
    "        X.append(np.append(x0, x1))\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    return X, Y\n",
    "\n",
    "# Xtrain, Ytrain = gen_data_cond(1000, cond_train, means_alt)\n",
    "# Xval, Yval = gen_data_cond(1000, cond_train, means_alt)\n",
    "# Xtest, Ytest = gen_data_cond(1000, cond_test, means_alt)\n",
    "Xbest, Ybest = gen_data_cond(1000, cond_best, means_alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data_n-1000_cor-0.8_dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.1dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.2dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.3dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.4dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.45dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.5dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.55dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.6dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.65dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.7dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.75dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.8dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.85dp-1.5_da-1.5.pckl\n",
      "test_data__cor-0.9dp-1.5_da-1.5.pckl\n"
     ]
    }
   ],
   "source": [
    "num_train = 1000\n",
    "dp = 1.5\n",
    "da = 1.5\n",
    "corr_train = 0.8\n",
    "corr_test = 0.5\n",
    "test_data_size = 50000\n",
    "\n",
    "# training set\n",
    "dfilename = 'train_data_n-' + str(num_train) + '_cor-' + str(corr_train) + '_dp-' + str(dp) + '_da-' + str(da) + '.pckl'\n",
    "means_alt = mean_alteration(means, dp, da)\n",
    "cond_test = get_conditional_prob(corr_train)\n",
    "Xtrain, Ytrain = gen_data_cond(num_train, cond_test, means_alt)\n",
    "with open('../synthetic_data/' + dfilename, 'w') as fp:\n",
    "   pickle.dump({'X':Xtrain, 'Y':Ytrain}, fp)\n",
    "print(dfilename)\n",
    "\n",
    "# test set with different correlation between attributes\n",
    "for corr_test in [0.1, 0.2, 0.3, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9]:\n",
    "    # test data\n",
    "    dfilename = 'test_data_' + '_cor-' + str(corr_test) + 'dp-' + str(dp) + '_da-' + str(da) + '.pckl'\n",
    "    if dfilename not in os.listdir('../synthetic_data/'):\n",
    "        means_alt = mean_alteration(means, dp, da)\n",
    "        cond_test = get_conditional_prob(corr_test)\n",
    "        Xtest, Ytest = gen_data_cond(test_data_size, cond_test, means_alt)\n",
    "        pickle.dump({'X':Xtest, 'Y':Ytest}, open('../synthetic_data/' + dfilename, 'w'))\n",
    "        print(dfilename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
